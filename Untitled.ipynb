{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "import numpy as np\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.stem import SnowballStemmer\n",
    "#from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from random import shuffle\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 300\n",
    "MAX_WORD_COUNT = 10000\n",
    "SKIP_TOP_WORDS = 20\n",
    "TEST_SPLIT = 0.2\n",
    "SEED = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(\n",
    "    path=\"reuters2.npz\",\n",
    "    num_words=MAX_WORD_COUNT,\n",
    "    skip_top=0,#SKIP_TOP_WORDS,\n",
    "    maxlen=MAX_SEQ_LENGTH,\n",
    "    test_split=TEST_SPLIT,\n",
    "    seed=SEED,\n",
    "    start_char=1,\n",
    "    oov_char=2,\n",
    "    index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('dataset/glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next(embeddings_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load reuters datasets, categories, and preprocesses the texts\"\"\"\n",
    "dataset_filename = \"dataset-TESTSPLIT_{}_NB_WORDS_{}_MAX_SEQ_LENGTH_{}.pkl\".format(\n",
    "    params['TEST_SPLIT'],\n",
    "    params['MAX_NB_WORDS'],\n",
    "    params['MAX_SEQUENCE_LENGTH']\n",
    ")\n",
    "\n",
    "print(\"Searching for dataset file {}\".format(dataset_filename))\n",
    "try:\n",
    "    file_content = pickle.load(open(dataset_filename, \"rb\"))\n",
    "except (OSError, IOError):\n",
    "    print(\"File not found\")\n",
    "else:\n",
    "    print(\"File found\")\n",
    "    return file_content\n",
    "\n",
    "download_dataset('stopwords')\n",
    "download_dataset('reuters')\n",
    "download_dataset('punkt')\n",
    "\n",
    "categories_to_idx = {c: i for i, c in enumerate(reuters.categories())}\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer('english')\n",
    "dataset = []\n",
    "for category in reuters.categories():\n",
    "    for file_id in reuters.fileids(category):\n",
    "        txt = reuters.raw(file_id)\n",
    "        txt_lower = txt.lower()\n",
    "        list_of_words = [stemmer.stem(w) for w in word_tokenize(txt_lower) if w not in stop_words]\n",
    "        dataset.append((\" \".join(list_of_words), categories_to_idx[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.datasets.reuters' has no attribute 'categories'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-bc4de5d51319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m params = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m\"NUM_CLASSES\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"MAX_SEQUENCE_LENGTH\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"MAX_NB_WORDS\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"EMBEDDING_DIM\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.datasets.reuters' has no attribute 'categories'"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"NUM_CLASSES\": len(reuters.categories()),\n",
    "    \"MAX_SEQUENCE_LENGTH\": 300,\n",
    "    \"MAX_NB_WORDS\": 10000,\n",
    "    \"EMBEDDING_DIM\": 25,\n",
    "    \"VALIDATION_SPLIT\": 0.1,\n",
    "    \"TEST_SPLIT\": 0.2,\n",
    "    \"NUM_LSTM\": 200,\n",
    "    \"NUM_DENSE\": 120,\n",
    "    \"RATE_DROP_LSTM\": 0.2,\n",
    "    \"RATE_DROP_DENSE\": 0.2,\n",
    "    \"ACTIVATION_FN\": 'relu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "        input_dim=MAX_NB_WORDS,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        #weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "#sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "#embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "#y1 = lstm_layer(embedded_sequences_2)\n",
    "#\n",
    "#merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(x1)#(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(NUM_CLASSES, activation='softmax')(merged)\n",
    "\n",
    "########################################\n",
    "## add class weight\n",
    "########################################\n",
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input], outputs=preds)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['acc']\n",
    ")\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
