{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "from random import shuffle\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.exceptions\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Nadam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk import word_tokenize, download as download_dataset\n",
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from metrics import *\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2w = {v: k for k, v in tokenizer.word_index.items()}\n",
    "idx2w.update({0: '<unk>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('dataset/glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "  'ACTIVATION_FN': 'relu',\n",
    "  'BATCH_SIZE': 64,\n",
    "  'EMBEDDING_DIM': 100,\n",
    "  'EPOCHS': 100,\n",
    "  'LR': 0.001,\n",
    "  'MAX_NB_WORDS': 10000,\n",
    "  'MAX_SEQUENCE_LENGTH': 300,\n",
    "  'NUM_CLASSES': 90,\n",
    "  'NUM_DENSE': 200,\n",
    "  'NUM_LSTM': 300,\n",
    "  'PATIENCE': 2,\n",
    "  'RATE_DROP_DENSE': 0.2,\n",
    "  'RATE_DROP_LSTM': 0.2,\n",
    "  'VALIDATION_SPLIT': 0.2,\n",
    "  'metrics_acc': 0.77012255713812516,\n",
    "  'metrics_f1': 0.77711240909775625,\n",
    "  'metrics_loss': 7.148816829092576,\n",
    "  'metrics_prec': 0.81774655224802373,\n",
    "  'metrics_rec': 0.76228632478632474,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data(params):\n",
    "    \"\"\"Load reuters datasets, categories, and preprocesses the texts\"\"\"\n",
    "    dataset_filename = \"dataset/dataset-TESTSPLIT_{}_NB_WORDS_{}_MAX_SEQ_LENGTH_{}.pkl\".format(\n",
    "        params['TEST_SPLIT'],\n",
    "        params['MAX_NB_WORDS'],\n",
    "        params['MAX_SEQUENCE_LENGTH']\n",
    "    )\n",
    "\n",
    "    print(\"Searching for dataset file {}\".format(dataset_filename))\n",
    "    try:\n",
    "        file_content = pickle.load(open(dataset_filename, \"rb\"))\n",
    "    except (OSError, IOError):\n",
    "        print(\"File not found\")\n",
    "    else:\n",
    "        print(\"File found\")\n",
    "        return file_content\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('reuters')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    categories_to_idx = {c: i for i, c in enumerate(reuters.categories())}\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for file_id in reuters.fileids():\n",
    "        txt = reuters.raw(file_id)\n",
    "        txt_lower = txt.lower()\n",
    "        list_of_words = [stemmer.stem(w) for w in word_tokenize(txt_lower) if w not in stop_words]\n",
    "        val = (\" \".join(list_of_words),\n",
    "                            [categories_to_idx[c] for c in reuters.categories(file_id)],\n",
    "                            file_id)\n",
    "        if \"train\" in file_id:\n",
    "            train_set.append(val)\n",
    "        else:\n",
    "            test_set.append(val)\n",
    "\n",
    "    mlb = MultiLabelBinarizer(list(range(len(categories_to_idx))))\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    shuffle(dataset)\n",
    "\n",
    "    # Make train and test sets\n",
    "    #test_limit = int(len(dataset)*params['TEST_SPLIT'])\n",
    "    #test_set = dataset[:test_limit]\n",
    "    #train_set = dataset[test_limit:]\n",
    "\n",
    "    # Fit the tokenizer on train texts\n",
    "    tokenizer = Tokenizer(num_words=params['MAX_NB_WORDS'])\n",
    "    tokenizer.fit_on_texts((txt for txt, category, _ in dataset))\n",
    "\n",
    "    # Convert them to indices and truncate them if they are too large\n",
    "    train_sequences = pad_sequences(\n",
    "        sequences=tokenizer.texts_to_sequences((txt for txt, category, _ in train_set)),\n",
    "        maxlen=params['MAX_SEQUENCE_LENGTH'])\n",
    "    test_sequences = pad_sequences(\n",
    "        sequences=tokenizer.texts_to_sequences((txt for txt, category, _ in test_set)),\n",
    "        maxlen=params['MAX_SEQUENCE_LENGTH'])\n",
    "    train_categories = mlb.fit_transform([categories for txt, categories, _ in train_set])\n",
    "    test_categories = mlb.fit_transform([categories for txt, categories, _ in test_set])\n",
    "    train_fileids = [fileid for txt, categories, fileid in train_set]\n",
    "    test_fileids = [fileid for txt, categories, fileid in test_set]\n",
    "    pickle.dump((train_sequences, train_categories, test_sequences, test_categories, train_fileids, test_fileids), open(dataset_filename, \"wb\"))\n",
    "\n",
    "    return train_sequences, train_categories, test_sequences, test_categories, train_fileids, test_fileids, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for dataset file dataset/dataset-TESTSPLIT_0.2_NB_WORDS_10000_MAX_SEQ_LENGTH_300.pkl\n",
      "File found\n"
     ]
    }
   ],
   "source": [
    "train_sequences, train_categories, test_sequences, test_categories, train_fileids, test_fileids, word_index = make_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences = np.concatenate((train_sequences, test_sequences), axis=0)\n",
    "all_categories = np.concatenate((train_categories, test_categories), axis=0)\n",
    "all_fileids = np.concatenate((train_fileids, test_fileids), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the recurrent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 300, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 300)               481200    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 90)                18090     \n",
      "=================================================================\n",
      "Total params: 1,559,490\n",
      "Trainable params: 1,559,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_model(params):\n",
    "    \"\"\"Builds the model\"\"\"\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=params['MAX_NB_WORDS'],\n",
    "        output_dim=params['EMBEDDING_DIM'],\n",
    "        # weights=[embedding_matrix],\n",
    "        input_length=params['MAX_SEQUENCE_LENGTH'])\n",
    "    lstm_layer = LSTM(params['NUM_LSTM'], dropout=params['RATE_DROP_LSTM'], recurrent_dropout=params['RATE_DROP_LSTM'])\n",
    "\n",
    "    sequence_1_input = Input(shape=(params['MAX_SEQUENCE_LENGTH'],), dtype='int32')\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    merged = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "    merged = Dense(params['NUM_DENSE'], activation=params['ACTIVATION_FN'])(merged)\n",
    "    merged = Dropout(params['RATE_DROP_DENSE'])(merged)\n",
    "\n",
    "    preds = Dense(params['NUM_CLASSES'], activation='softmax')(merged)\n",
    "\n",
    "    model = Model(inputs=[sequence_1_input], outputs=preds)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc']\n",
    "    )\n",
    "    model.summary()\n",
    "    model_bis = Model(inputs=[sequence_1_input], outputs=merged)\n",
    "    return model, model_bis\n",
    "model, model_bis = make_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"model-20171214-023454.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_representations = model_bis.predict(all_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump((all_representations, all_fileids), open(\"model-20171214-representations.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['training/7275', 'test/15190', 'training/6208', ..., 'test/15695',\n",
       "       'training/7359', 'training/1562'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
